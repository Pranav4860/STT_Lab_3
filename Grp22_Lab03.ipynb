{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Google Drive and fetching Data"
      ],
      "metadata": {
        "id": "qORzrGBFpN2I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ilA2WRBlweO",
        "outputId": "53b49aae-b176-4654-c2e0-9970a467380c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa"
      ],
      "metadata": {
        "id": "lUajZ2n5yTbK"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CV_Pranav=pd.read_csv(\"/content/drive/MyDrive/Annotations/CV_253.csv\")\n",
        "CV_Soham=pd.read_csv(\"/content/drive/MyDrive/Annotations/CV_Soham.csv\")\n",
        "CV_Vansh=pd.read_csv(\"/content/drive/MyDrive/Annotations/CV_Vansh.csv\")\n",
        "NLP_Pranav=pd.read_csv(\"/content/drive/MyDrive/Annotations/NLP_253.csv\")\n",
        "NLP_Soham=pd.read_csv(\"/content/drive/MyDrive/Annotations/soham_annotations.csv\")"
      ],
      "metadata": {
        "id": "MCaALAwpxtPT"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categories"
      ],
      "metadata": {
        "id": "MPzDW8E8zN6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cats = [\n",
        "    \"NOUN\",\n",
        "    \"PROPN\",\n",
        "    \"VERB\",\n",
        "    \"ADJ\",\n",
        "    \"ADV\",\n",
        "    \"ADP\",\n",
        "    \"PRON\",\n",
        "    \"DET\",\n",
        "    \"CONJ\",\n",
        "    \"PART\",\n",
        "    \"PRON_WH\",\n",
        "    \"PART_NEG\",\n",
        "    \"NUM\",\n",
        "    \"X\"\n",
        "]"
      ],
      "metadata": {
        "id": "e54iV_dIyz_f"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing of Data"
      ],
      "metadata": {
        "id": "9tK4QuFu11HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data1,data2,cats):\n",
        "  return_list=[[],[]]\n",
        "  for i in range(len(data1)):\n",
        "    dic1={}\n",
        "    dic2={}\n",
        "    lbl1=json.loads(data1[i])\n",
        "    lbl2=json.loads(data2[i])\n",
        "    for i in lbl1:\n",
        "      dic1[i[\"text\"].strip()]=i[\"labels\"][0].strip('\"')\n",
        "    for i in lbl2:\n",
        "      dic2[i[\"text\"].strip()]=i[\"labels\"][0].strip('\"')\n",
        "    for word,lbl1 in dic1.items():\n",
        "      if word in dic2:\n",
        "        lbl2=dic2[word]\n",
        "        dic2.pop(word)\n",
        "      else:\n",
        "        lbl2=\"X\"\n",
        "      return_list[0].append(cats.index(lbl1))\n",
        "      return_list[1].append(cats.index(lbl2))\n",
        "    for word,lbl2 in dic2.items():\n",
        "      lbl1=\"X\"\n",
        "      return_list[0].append(cats.index(lbl1))\n",
        "      return_list[1].append(cats.index(lbl2))\n",
        "  return return_list\n",
        "\n"
      ],
      "metadata": {
        "id": "8-fg1JMvzSsL"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Cohen's Kappa"
      ],
      "metadata": {
        "id": "rSNVJvSSt2M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prep_data= preprocess(NLP_Pranav[\"label\"].tolist(), NLP_Soham[\"label\"].tolist(),cats)\n",
        "print(\"Rater Data:\")\n",
        "print(\"Pranav:\")\n",
        "print(prep_data[0])\n",
        "print(\"Soham:\")\n",
        "print(prep_data[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTbMFua90m6L",
        "outputId": "2124d34b-3e95-4e56-a33a-7723deb62539"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rater Data:\n",
            "Pranav:\n",
            "[0, 0, 0, 1, 1, 1, 2, 2, 2, 4, 5, 5, 6, 6, 12, 8, 13, 13, 13, 13, 13, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3, 3, 4, 5, 5, 6, 6, 8, 13, 13, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 5, 5, 5, 5, 5, 8, 12, 13, 13, 13, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 5, 8, 12, 13, 13, 13, 13, 13, 13, 0, 0, 1, 1, 1, 2, 2, 2, 2, 5, 5, 12, 13, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 5, 5, 5, 5, 6, 6, 8, 13, 0, 1, 1, 1, 1, 2, 3, 3, 5, 5, 5, 6, 13, 0, 0, 0, 1, 1, 1, 2, 3, 3, 5, 5, 5, 6, 13, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 5, 5, 5, 6, 6, 6, 13, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 3, 5, 5, 6, 6, 13, 13, 13, 13, 13, 13, 0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 5, 5, 5, 7, 8, 12, 12, 5, 13, 13, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 5, 5, 5, 5, 5, 6, 7, 2, 13, 13, 13, 13, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 4, 4, 5, 5, 5, 6, 0, 9, 13, 13, 13, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 5, 5, 5, 5, 5, 5, 2, 5, 13, 13, 13, 0, 1, 1, 1, 1, 1, 2, 3, 5, 5, 12, 13, 13, 13, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 5, 5, 5, 5, 5, 6, 6, 5, 0, 13, 13, 13, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 8, 8, 3, 0, 0, 13, 13, 13, 0, 2, 9, 0, 5, 1, 2, 8, 1, 1, 5, 4, 4, 0, 5, 0, 6, 2, 2, 9, 13, 13, 13, 13, 0, 0, 1, 5, 1, 3, 3, 7, 3, 0, 4, 6, 5, 0, 2, 9, 2, 13, 13, 13, 13, 13, 13, 13, 1, 1, 1, 4, 11, 0, 5, 0, 5, 2, 2, 9, 6, 4, 3, 3, 0, 2, 13, 13, 13]\n",
            "Soham:\n",
            "[0, 0, 0, 1, 1, 3, 2, 13, 13, 4, 5, 5, 6, 6, 7, 8, 2, 9, 5, 2, 9, 0, 0, 0, 0, 0, 0, 1, 13, 13, 9, 3, 3, 4, 5, 5, 6, 5, 8, 4, 2, 0, 0, 1, 0, 1, 0, 1, 1, 1, 13, 13, 13, 13, 3, 5, 5, 5, 5, 5, 8, 12, 0, 2, 9, 13, 0, 13, 13, 13, 1, 13, 2, 2, 2, 13, 13, 13, 9, 5, 8, 12, 1, 1, 1, 0, 1, 13, 0, 0, 1, 1, 1, 2, 9, 9, 9, 5, 5, 12, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2, 2, 5, 5, 5, 8, 6, 5, 8, 13, 0, 1, 1, 3, 0, 2, 4, 3, 5, 5, 5, 6, 13, 0, 1, 0, 1, 1, 1, 2, 2, 4, 5, 5, 5, 6, 13, 0, 0, 0, 1, 1, 1, 1, 2, 9, 2, 9, 5, 5, 5, 6, 6, 6, 13, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 3, 5, 13, 6, 7, 13, 5, 5, 3, 5, 13, 0, 0, 0, 1, 1, 1, 2, 2, 3, 4, 5, 5, 5, 12, 8, 12, 12, 5, 2, 13, 0, 3, 0, 0, 0, 0, 0, 0, 0, 9, 13, 3, 3, 5, 5, 5, 5, 13, 7, 7, 2, 0, 2, 2, 13, 0, 0, 0, 0, 1, 0, 1, 1, 2, 13, 9, 3, 9, 9, 5, 5, 5, 7, 0, 9, 2, 2, 13, 0, 13, 0, 0, 1, 1, 1, 2, 2, 2, 3, 7, 5, 13, 5, 5, 5, 5, 2, 5, 0, 9, 13, 0, 1, 1, 1, 1, 1, 2, 3, 13, 5, 7, 5, 9, 13, 0, 1, 0, 0, 0, 0, 1, 0, 1, 2, 2, 4, 3, 5, 5, 5, 13, 13, 6, 6, 5, 4, 5, 0, 13, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 6, 0, 5, 5, 5, 5, 8, 8, 13, 4, 0, 1, 3, 13, 0, 2, 3, 0, 5, 13, 2, 8, 1, 13, 5, 9, 4, 0, 5, 0, 5, 2, 2, 2, 1, 1, 1, 13, 0, 0, 1, 5, 13, 5, 13, 12, 3, 0, 9, 6, 5, 0, 2, 2, 2, 1, 0, 0, 0, 5, 5, 13, 1, 1, 1, 6, 11, 0, 5, 0, 5, 2, 2, 3, 13, 4, 13, 3, 0, 2, 6, 3, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cohen's Kappa"
      ],
      "metadata": {
        "id": "yPUmdXwbvhR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cohen's Kappa\",cohen_kappa_score(prep_data[0],prep_data[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV8VhcqEu0o4",
        "outputId": "d73bab75-0328-41f3-e6e3-13d3cddf179d"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa 0.6338404688747817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CV"
      ],
      "metadata": {
        "id": "gNvl4IipyK3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cv_prep(df1,df2,df3):\n",
        "  return_list=[]\n",
        "  for i in range(len(df1)):\n",
        "    truck_count=0\n",
        "    no_truck_count=0\n",
        "    if df1[i]==\"Truck\":\n",
        "      truck_count+=1\n",
        "    else:\n",
        "      no_truck_count+=1\n",
        "    if df2[i]==\"Truck\":\n",
        "      truck_count+=1\n",
        "    else:\n",
        "      no_truck_count+=1\n",
        "    if df3[i]==\"Truck\":\n",
        "      truck_count+=1\n",
        "    else:\n",
        "      no_truck_count+=1\n",
        "    return_list.append([truck_count,no_truck_count])\n",
        "  return return_list"
      ],
      "metadata": {
        "id": "GJy4ECNHv6Us"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_Pranav=CV_Pranav[\"choice\"].tolist()\n",
        "prep_Soham=CV_Soham[\"choice\"].tolist()\n",
        "prep_Vansh=CV_Vansh[\"choice\"].tolist()"
      ],
      "metadata": {
        "id": "RPvUgtC59hyl"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_list=cv_prep(prep_Pranav,prep_Soham,prep_Vansh)\n",
        "prep_arr=np.array(prep_list)\n",
        "f_kappa=fleiss_kappa(prep_arr,method=\"fleiss\")"
      ],
      "metadata": {
        "id": "iWnmx7EJ91c7"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f_kappa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWfOQu3yFXba",
        "outputId": "934935ce-c78f-490f-83d6-126fce18b7f9"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8382749326145548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summary"
      ],
      "metadata": {
        "id": "AZOuk2r_Xky_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Pre-Processed the annotated data of NLP. The data was annotated by Pranav Jigar Thakkar and Soham Srivastva.\n",
        "2. Used the Function \"cohen_kappa_score\" to calculate Cohen's Kappa of NLP data.\n",
        "3. Got Cohen's Kappa as 0.6338404688747817 which signifies good agreement in the labelling of annotators and a relatively consistent labeling process .\n",
        "4. Pre-Processed the annotated data of CV dataset. The data was annotated by Pranav Jigar Thakkar, Soham Srivastva and Vansh.\n",
        "5. Used the Function \"fleiss_kappa\" from \"statsmodels.stats.inter_rater\" to calculate Fleiss Kappa of CV data.\n",
        "6. Got Cohen's Kappa as 0.8382749326145548 which suggests strong agreement in the labelling of annotators and a consistent and reliable labeling process across the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JRQ1zC42Xmu9"
      }
    }
  ]
}